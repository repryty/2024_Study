{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 가치 함수 구하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래는 우리가 다루게 될 환경입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the 4 * 4 grid environment\n",
    "class GridEnv:\n",
    "    def __init__(self):\n",
    "        self.state_space = [(i, j) for i in range(4) for j in range(4)]\n",
    "        self.action  _space = ['up', 'down', 'left', 'right']\n",
    "\n",
    "        self.grid = [\n",
    "            [0, 0, 0, 0], \n",
    "            [0, 0, 0, 0], \n",
    "            [0, 0, 0, 0], \n",
    "            [0, 0, 0, 0]\n",
    "        ]\n",
    "        self.start = (3, 0)\n",
    "        self.goal = (0, 3)\n",
    "        self.rewards = [\n",
    "            [-1, -1, -1, 1],\n",
    "            [-1, -1, -1, -1], \n",
    "            [-1, -1, -1, -1], \n",
    "            [-1, -1, -1, -1]\n",
    "        ]\n",
    "        self.done = [\n",
    "            [0, 0, 0, 1], \n",
    "            [0, 0, 0, 0], \n",
    "            [0, 0, 0, 0], \n",
    "            [0, 0, 0, 0]\n",
    "        ]\n",
    "\n",
    "    def transition(self, state, action):\n",
    "        x, y = state\n",
    "        if action == 'up':\n",
    "            x = max(x - 1, 0)\n",
    "        elif action == 'down':\n",
    "            x = min(x + 1, 3)\n",
    "        elif action == 'left':\n",
    "            y = max(y - 1, 0)\n",
    "        elif action == 'right':\n",
    "            y = min(y + 1, 3)\n",
    "        return (x, y)\n",
    "    \n",
    "    def transition_prob(self, next_state, state, action):\n",
    "        return next_state == self.transition(state, action)\n",
    "    \n",
    "    def reward(self, state, action):\n",
    "        next_state = self.transition(state, action)\n",
    "        x, y = next_state\n",
    "        return self.rewards[x][y]\n",
    "\n",
    "    def reset(self):\n",
    "        self.grid = [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]\n",
    "        self.state = self.start\n",
    "        return self.start\n",
    "\n",
    "    def step(self, action):\n",
    "        next_state = self.transition(self.state, action)\n",
    "        reward = self.reward(self.state, action)\n",
    "        done = self.done[next_state[0]][next_state[1]]\n",
    "        self.state = next_state\n",
    "        return next_state, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state: (3, 0)\n",
      "Chose action: down\n",
      "New state: (3, 0)\n",
      "Reward: -1\n",
      "Chose action: left\n",
      "New state: (3, 0)\n",
      "Reward: -1\n",
      "Chose action: down\n",
      "New state: (3, 0)\n",
      "Reward: -1\n",
      "Chose action: right\n",
      "New state: (3, 1)\n",
      "Reward: -1\n",
      "Chose action: right\n",
      "New state: (3, 2)\n",
      "Reward: -1\n",
      "Chose action: left\n",
      "New state: (3, 1)\n",
      "Reward: -1\n",
      "Chose action: left\n",
      "New state: (3, 0)\n",
      "Reward: -1\n",
      "Chose action: down\n",
      "New state: (3, 0)\n",
      "Reward: -1\n",
      "Chose action: left\n",
      "New state: (3, 0)\n",
      "Reward: -1\n",
      "Chose action: left\n",
      "New state: (3, 0)\n",
      "Reward: -1\n",
      "Chose action: up\n",
      "New state: (2, 0)\n",
      "Reward: -1\n",
      "Chose action: up\n",
      "New state: (1, 0)\n",
      "Reward: -1\n",
      "Chose action: down\n",
      "New state: (2, 0)\n",
      "Reward: -1\n",
      "Chose action: up\n",
      "New state: (1, 0)\n",
      "Reward: -1\n",
      "Chose action: down\n",
      "New state: (2, 0)\n",
      "Reward: -1\n",
      "Chose action: up\n",
      "New state: (1, 0)\n",
      "Reward: -1\n",
      "Chose action: down\n",
      "New state: (2, 0)\n",
      "Reward: -1\n",
      "Chose action: down\n",
      "New state: (3, 0)\n",
      "Reward: -1\n",
      "Chose action: up\n",
      "New state: (2, 0)\n",
      "Reward: -1\n",
      "Chose action: left\n",
      "New state: (2, 0)\n",
      "Reward: -1\n",
      "Chose action: down\n",
      "New state: (3, 0)\n",
      "Reward: -1\n",
      "Chose action: down\n",
      "New state: (3, 0)\n",
      "Reward: -1\n",
      "Chose action: up\n",
      "New state: (2, 0)\n",
      "Reward: -1\n",
      "Chose action: right\n",
      "New state: (2, 1)\n",
      "Reward: -1\n",
      "Chose action: down\n",
      "New state: (3, 1)\n",
      "Reward: -1\n",
      "Chose action: left\n",
      "New state: (3, 0)\n",
      "Reward: -1\n",
      "Chose action: down\n",
      "New state: (3, 0)\n",
      "Reward: -1\n",
      "Chose action: down\n",
      "New state: (3, 0)\n",
      "Reward: -1\n",
      "Chose action: right\n",
      "New state: (3, 1)\n",
      "Reward: -1\n",
      "Chose action: up\n",
      "New state: (2, 1)\n",
      "Reward: -1\n",
      "Chose action: left\n",
      "New state: (2, 0)\n",
      "Reward: -1\n",
      "Chose action: up\n",
      "New state: (1, 0)\n",
      "Reward: -1\n",
      "Chose action: down\n",
      "New state: (2, 0)\n",
      "Reward: -1\n",
      "Chose action: up\n",
      "New state: (1, 0)\n",
      "Reward: -1\n",
      "Chose action: right\n",
      "New state: (1, 1)\n",
      "Reward: -1\n",
      "Chose action: down\n",
      "New state: (2, 1)\n",
      "Reward: -1\n",
      "Chose action: left\n",
      "New state: (2, 0)\n",
      "Reward: -1\n",
      "Chose action: down\n",
      "New state: (3, 0)\n",
      "Reward: -1\n",
      "Chose action: left\n",
      "New state: (3, 0)\n",
      "Reward: -1\n",
      "Chose action: down\n",
      "New state: (3, 0)\n",
      "Reward: -1\n",
      "Chose action: right\n",
      "New state: (3, 1)\n",
      "Reward: -1\n",
      "Chose action: up\n",
      "New state: (2, 1)\n",
      "Reward: -1\n",
      "Chose action: right\n",
      "New state: (2, 2)\n",
      "Reward: -1\n",
      "Chose action: up\n",
      "New state: (1, 2)\n",
      "Reward: -1\n",
      "Chose action: down\n",
      "New state: (2, 2)\n",
      "Reward: -1\n",
      "Chose action: right\n",
      "New state: (2, 3)\n",
      "Reward: -1\n",
      "Chose action: left\n",
      "New state: (2, 2)\n",
      "Reward: -1\n",
      "Chose action: right\n",
      "New state: (2, 3)\n",
      "Reward: -1\n",
      "Chose action: right\n",
      "New state: (2, 3)\n",
      "Reward: -1\n",
      "Chose action: up\n",
      "New state: (1, 3)\n",
      "Reward: -1\n",
      "Chose action: right\n",
      "New state: (1, 3)\n",
      "Reward: -1\n",
      "Chose action: down\n",
      "New state: (2, 3)\n",
      "Reward: -1\n",
      "Chose action: left\n",
      "New state: (2, 2)\n",
      "Reward: -1\n",
      "Chose action: up\n",
      "New state: (1, 2)\n",
      "Reward: -1\n",
      "Chose action: left\n",
      "New state: (1, 1)\n",
      "Reward: -1\n",
      "Chose action: up\n",
      "New state: (0, 1)\n",
      "Reward: -1\n",
      "Chose action: right\n",
      "New state: (0, 2)\n",
      "Reward: -1\n",
      "Chose action: left\n",
      "New state: (0, 1)\n",
      "Reward: -1\n",
      "Chose action: up\n",
      "New state: (0, 1)\n",
      "Reward: -1\n",
      "Chose action: left\n",
      "New state: (0, 0)\n",
      "Reward: -1\n",
      "Chose action: right\n",
      "New state: (0, 1)\n",
      "Reward: -1\n",
      "Chose action: right\n",
      "New state: (0, 2)\n",
      "Reward: -1\n",
      "Chose action: up\n",
      "New state: (0, 2)\n",
      "Reward: -1\n",
      "Chose action: right\n",
      "New state: (0, 3)\n",
      "Reward: 1\n"
     ]
    }
   ],
   "source": [
    "# Sample code for running the environment\n",
    "import random\n",
    "\n",
    "env = GridEnv()\n",
    "state = env.reset()\n",
    "done = False\n",
    "print(\"Initial state:\", state)\n",
    "while not done:\n",
    "    action = random.choice(env.action_space)\n",
    "    print(\"Chose action:\", action)\n",
    "    state, reward, done = env.step(action)\n",
    "    print(\"New state:\", state)\n",
    "    print(\"Reward:\", reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저, 아무 전략이나 만들어봅시다. 단순하게 무조건 위로만 가는 전략을 생각해볼까요~?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "PI = {s: 'up' for s in env.state_space}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_policy(env, PI):\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            if (i, j) == env.goal:\n",
    "                print(\" G \", end='\\t')\n",
    "            else:\n",
    "                print(PI[(i, j)], end='\\t')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "up\tup\tup\t G \t\n",
      "up\tup\tup\tup\t\n",
      "up\tup\tup\tup\t\n",
      "up\tup\tup\tup\t\n"
     ]
    }
   ],
   "source": [
    "visualize_policy(env, PI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 가치 평가 단계\n",
    "우리가 가지고 있는 전략이 얼마나 좋은지 평가하려면, 가치 함수를 구하면 됩니다.\n",
    "\n",
    "수업 시간 때 다룬 것처럼, 가치 함수는 이 전략을 따라하면 기대할 수 있는 보상의 총합을 구해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Evaluation\n",
    "def policy_evaluation(env, PI, gamma=1):\n",
    "    V = {s: 0 for s in env.state_space}\n",
    "    for _ in range(1000):\n",
    "        # delta = 0\n",
    "        for s in env.state_space:\n",
    "            action = PI[s]\n",
    "            next_state=env.transition(s, action)\n",
    "            reward=env.reward(s,action)\n",
    "            V[s] = reward+gamma*V[next_state]\n",
    "\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1000.00\t-1000.00\t-1000.00\t1000.00\t\n",
      "-1001.00\t-1001.00\t-1001.00\t1001.00\t\n",
      "-1002.00\t-1002.00\t-1002.00\t1000.00\t\n",
      "-1003.00\t-1003.00\t-1003.00\t999.00\t\n"
     ]
    }
   ],
   "source": [
    "V = policy_evaluation(env, PI)\n",
    "# Visualize the value function\n",
    "def visualize_value_function(V):\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            print(f\"{V[(i, j)]:.2f}\", end='\\t')\n",
    "        print()\n",
    "\n",
    "visualize_value_function(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정책 개선 단계\n",
    "\n",
    "주어진 상태의 가치는 이 상태가 얼마나 좋은지를 알려줍니다.\n",
    "\n",
    "따라서, 가치가 큰 방향으로 이동하는 새로운 전략을 세우면, 더 좋은 전략이 될 수 있겠네요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.transition(state, 'up')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Improvement\n",
    "def policy_improvement(env, V, PI, gamma=1):\n",
    "    for s in env.state_space:\n",
    "        max_v = float('-inf')\n",
    "        for a in env.action_space:\n",
    "            # max_v=max(V[env.transition(s, a)], max_v)\n",
    "            next_state=env.transition(s,a)\n",
    "            if max_v<V[next_state]:\n",
    "                max_v=V[next_state]\n",
    "                PI[s]=a\n",
    "    return PI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "up\tup\tright\t G \t\n",
      "up\tup\tright\tright\t\n",
      "up\tup\tright\tup\t\n",
      "up\tup\tright\tup\t\n"
     ]
    }
   ],
   "source": [
    "PI = policy_improvement(env, V, PI)\n",
    "visualize_policy(env, PI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정책 반복법 시행하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "up\tup\tright\t G \t\n",
      "up\tup\tright\tright\t\n",
      "up\tup\tright\tup\t\n",
      "up\tup\tright\tup\t\n",
      "-1000.00\t-1000.00\t-1000.00\t1000.00\t\n",
      "-1001.00\t-1001.00\t-1001.00\t1001.00\t\n",
      "-1002.00\t-1002.00\t-1002.00\t1000.00\t\n",
      "-1003.00\t-1003.00\t-1003.00\t999.00\t\n",
      "\n",
      "up\tright\tup\t G \t\n",
      "up\tup\tup\tup\t\n",
      "up\tup\tup\tup\t\n",
      "up\tup\tup\tup\t\n",
      "-1000.00\t-1000.00\t-998.00\t-1000.00\t\n",
      "-1001.00\t-1001.00\t-1000.00\t-1000.00\t\n",
      "-1002.00\t-1002.00\t-1001.00\t-1001.00\t\n",
      "-1003.00\t-1003.00\t-1002.00\t-1002.00\t\n",
      "\n",
      "up\tup\tup\t G \t\n",
      "up\tup\tup\tright\t\n",
      "up\tup\tup\tup\t\n",
      "up\tup\tup\tup\t\n",
      "-1000.00\t-1000.00\t-1000.00\t-1001.00\t\n",
      "-1001.00\t-1001.00\t-1001.00\t-1000.00\t\n",
      "-1002.00\t-1002.00\t-1002.00\t-1001.00\t\n",
      "-1003.00\t-1003.00\t-1003.00\t-1002.00\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PI = {s: 'up' for s in env.state_space}\n",
    "for _ in range(3):\n",
    "    V = policy_evaluation(env, PI)\n",
    "    PI = policy_improvement(env, V, PI)\n",
    "    visualize_policy(env, PI)\n",
    "    visualize_value_function(V)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rllib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
