{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 강화 학습 환경 복습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gymnasium[classic-control]\n",
      "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gymnasium[classic-control]) (1.23.2)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gymnasium[classic-control]) (7.1.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gymnasium[classic-control]) (4.3.0)\n",
      "Collecting cloudpickle>=1.2.0\n",
      "  Downloading cloudpickle-3.0.0-py3-none-any.whl (20 kB)\n",
      "Collecting farama-notifications>=0.0.1\n",
      "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
      "Collecting pygame>=2.1.3\n",
      "  Downloading pygame-2.5.2-cp39-cp39-win_amd64.whl (10.8 MB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from importlib-metadata>=4.8.0->gymnasium[classic-control]) (3.18.1)\n",
      "Installing collected packages: farama-notifications, cloudpickle, pygame, gymnasium\n",
      "Successfully installed cloudpickle-3.0.0 farama-notifications-0.0.4 gymnasium-0.29.1 pygame-2.5.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\users\\user\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium[classic-control]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 강화 학습 문제를 직접 풀어낼 정책 정의\n",
    "\n",
    "강화 학습에서는 어떤 함수를 학습하고자 하는 걸까요? 에이전트 안에는 상태 관측값(입력)을 받고 그것을 앞으로 취해야 할 최적의 행동(출력)에 매핑하는 함수가 있습니다. 예를 들어, 미로 속 에이전트의 현재 상태가 $(2, 3)$ 좌표라면, 에이전트 안의 함수는 이 입력값을 \"오른쪽으로 이동\"이라는 출력값에 매핑하는 것이 될 수 있습니다. 이 함수를 $\\pi$라고 한다면, 아래와 같이 수식으로 쓸 수 있습니다.\n",
    "$$\n",
    "\\pi((2, 3)) = \\text{\"오른쪽으로 이동\"}\n",
    "$$\n",
    "강화 학습 용어로 이 함수를 정책(policy)이라고 부릅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state):\n",
    "    x_pos, y_pos = state\n",
    "    if x_pos == 2 and y_pos == 3:\n",
    "        return +1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 강화 학습이 돌아가는 환경의 코드 복습\n",
    "\n",
    "1. 인공지능 모델은 환경의 현재 상태(state)를 관찰할 수 있습니다. 미로 찾기 문제에서 환경의 현재 상태란 미로 속 현재 위치를 의미합니다. 예를 들어, 모델이 미로의 $(2, 3)$ 좌표에 있다면, 이 좌표는 현재 상태를 나타냅니다.\n",
    "\n",
    "2. 인공지능 모델은 관찰된 상태로부터 앞으로 취할 행동(action)을 결정합니다. 양갈래 길 중에서 어디로 갈지 결정하는 것 등이 그 예시가 될 수 있습니다.\n",
    "\n",
    "3. 환경은 상태를 변경(transition)시키고 그 행동에 대한 보상(reward)을 생성합니다. 인공지능 모델은 그 상태와 보상을 다 받습니다. 미로 찾기 문제에서 환경의 변화란 인공지능 모델의 (앞선 결정에 따른) 미로 속 위치 변화를 의미합니다. 예를 들어, '오른쪽으로 이동' 행동을 취하면, 에이전트의 위치 좌표가 $(2, 3)$에서 $(2, 4)$로 바뀔 수 있습니다. 보상은 출구를 찾았을 때 주어지는 경품이나 막다른 길에 도달했을 때 받는 페널티 등을 생각해 볼 수 있습니다.\n",
    "\n",
    "4.  이 새로운 정보(환경의 변화와 이에 따른 보상)를 사용하여 인공지능은 그런 행동이 좋아 그걸 반복해야 하는지, 또는 좋지 않아 회피해야 하는지 결정할 수 있습니다. 완료될 때까지 (done) 이 관측-행동-보상 사이클은 계속됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make('MountainCar-v0')\n",
    "state, _ = env.reset()\n",
    "print(\"Initial state:\", state)\n",
    "\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "    action = policy(state) # Step 1-2: Observes the state and chooses an action\n",
    "    print(\"Chose action:\", action)\n",
    "    state, reward, done, _, _ = env.step(action) # Step 3: Environment returns the next state and reward\n",
    "    total_reward += reward\n",
    "    print(\"New state:\", state)\n",
    "    print(\"Reward:\", reward)\n",
    "    if total_reward < -200:\n",
    "        break\n",
    "\n",
    "print(\"Final state:\", state)\n",
    "print(\"Total reward:\", total_reward)\n",
    "env.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 환경 코드 예시 살펴보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 복도를 걸어다니며 배정된 방까지 이동하는 환경\n",
    "강화 학습 환경을 더 직관적으로 이해하기 위해 직접 환경 코드를 만들어 봅시다. 강화 학습의 환경을 만들기 위해서는 먼저 상태 공간 $S$와 행동 공간 $A$를 정의해야 합니다.\n",
    "\n",
    "예를 들어, 일자형 복도에서 배정받은 방을 찾아 돌아다니는 환경을 생각해봅시다. 여러분은 왼쪽 또는 오른쪽으로 이동할 수 있습니다. 이 환경에서 상태와 행동은 아래와 같이 표현할 수 있습니다.\n",
    "\\begin{equation}\n",
    "S = \\{(i, j): i, j \\in \\{\\text{Room 101}, \\cdots, \\text{Room 106}\\}\\}, \\quad A = \\{\\text{left}, \\text{right}\\}\n",
    "\\end{equation}\n",
    "즉, 환경의 상태는 현재 에이전트의 위치 뿐 아니라 배정받은 방이 어디인지도 표현할 수 있어야 합니다.\n",
    "이를 코드로 보면 아래와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rooms = list(range(101, 107)) # [101, ..., 106]\n",
    "state_space = [(i, j) for i in rooms for j in rooms]\n",
    "action_space = [-1, 1] # left, right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "맨 끝 방에서는 반대 방향으로만 이동할 수 있고, 복도 밖으로 이동하려고 해도 벽에 부딪혀 더 움직이지 못합니다. 이를 코드로 구현하면 아래와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition(state, action):\n",
    "    current_location, my_room = state\n",
    "    next_location = current_location + action # moves with prob. 1\n",
    "    next_location = max(next_location, 101) # can't move left\n",
    "    next_location = min(next_location, 106) # can't move right\n",
    "    next_state = (next_location, my_room)\n",
    "    return next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음으로 보상을 정의합니다. 여러분이 배정받은 초록색 방에 도착하면 1의 보상을 받고 환경은 종료됩니다. 그 외의 경우에는 보상이 없습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function(state, action):\n",
    "    next_state = transition(state, action)\n",
    "    next_location, my_room = next_state\n",
    "    if next_location == my_room:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 이 환경을 코드로 구현하려면, 크게 두 가지 함수를 정의해야합니다. 첫째, 환경을 생성할 때 에이전트가 처음 관찰할 상태를 제공하는 함수를 만들어야 합니다. 둘째, 에이전트가 환경을 선택했을 때, 다음 상태와 보상을 제공하는 함수를 만들어야 합니다. 아래 코드는 이 두 가지를 각각 $\\texttt{reset}$과 $\\texttt{step}$ 함수에 구현한 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Defines our corridor environment\n",
    "class CorridorEnv:\n",
    "    def __init__(self):\n",
    "        rooms = list(range(101, 107))\n",
    "        self.state_space = [(i, j) for i in rooms for j in rooms]\n",
    "        self.action_space = [-1, 1, 3]\n",
    "\n",
    "    # 위의 코드와 동일!\n",
    "    def transition(self, state, action):\n",
    "        current_location, my_room = state\n",
    "        next_location = current_location + action \n",
    "        next_location = max(next_location, 101) \n",
    "        next_location = min(next_location, 106)\n",
    "        next_state = (next_location, my_room)\n",
    "        return next_state\n",
    "\n",
    "    # 위의 코드와 동일!\n",
    "    def reward_function(self, state, action):\n",
    "        next_state = transition(state, action)\n",
    "        next_location, my_room = next_state\n",
    "        if next_location == my_room:\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "    def reset(self):\n",
    "        same_start_and_end = False\n",
    "        while not same_start_and_end:\n",
    "            state = random.choice(self.state_space)\n",
    "            same_start_and_end = state[0] != state[1]\n",
    "        self.state = state\n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        next_state = self.transition(self.state, action)\n",
    "        reward = self.reward_function(self.state, action)\n",
    "        if action==3: done = (next_state[0]==next_state[1])\n",
    "        self.state = next_state\n",
    "        return next_state, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state: (101, 103)\n",
      "Chose action: -1\n",
      "New state: (101, 103)\n",
      "Reward: 0\n",
      "==========\n",
      "Chose action: 0\n",
      "New state: (101, 103)\n",
      "Reward: 0\n",
      "==========\n",
      "Chose action: 0\n",
      "New state: (101, 103)\n",
      "Reward: 0\n",
      "==========\n",
      "Chose action: -1\n",
      "New state: (101, 103)\n",
      "Reward: 0\n",
      "==========\n",
      "Chose action: 0\n",
      "New state: (101, 103)\n",
      "Reward: 0\n",
      "==========\n",
      "Chose action: 1\n",
      "New state: (101, 103)\n",
      "Reward: 0\n",
      "==========\n",
      "Chose action: -1\n",
      "New state: (101, 103)\n",
      "Reward: 0\n",
      "==========\n",
      "Chose action: 1\n",
      "New state: (102, 103)\n",
      "Reward: 0\n",
      "==========\n",
      "Chose action: 1\n",
      "New state: (101, 103)\n",
      "Reward: 1\n",
      "==========\n",
      "Chose action: 1\n",
      "New state: (101, 103)\n",
      "Reward: 0\n",
      "==========\n",
      "Chose action: -1\n",
      "New state: (101, 103)\n",
      "Reward: 0\n",
      "==========\n",
      "Chose action: 1\n",
      "New state: (101, 103)\n",
      "Reward: 0\n",
      "==========\n",
      "Chose action: 1\n",
      "New state: (101, 103)\n",
      "Reward: 0\n",
      "==========\n",
      "Chose action: 0\n",
      "New state: (101, 103)\n",
      "Reward: 0\n",
      "==========\n",
      "Chose action: -1\n",
      "New state: (102, 103)\n",
      "Reward: 0\n",
      "==========\n",
      "Chose action: -1\n",
      "New state: (101, 103)\n",
      "Reward: 0\n",
      "==========\n",
      "Chose action: -1\n",
      "New state: (102, 103)\n",
      "Reward: 0\n",
      "==========\n",
      "Chose action: -1\n",
      "New state: (103, 103)\n",
      "Reward: 0\n",
      "==========\n",
      "Chose action: 0\n",
      "New state: (103, 103)\n",
      "Reward: 1\n",
      "==========\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "# Sample code for running the environment\n",
    "env = DrunkenCorridorEnv()\n",
    "state = env.reset()\n",
    "done = False\n",
    "print(\"Initial state:\", state)\n",
    "a=0\n",
    "while not done:\n",
    "    a+=1\n",
    "    action = random.choice(env.action_space)\n",
    "    print(\"Chose action:\", action)\n",
    "    state, reward, done = env.step(action)\n",
    "    print(\"New state:\", state)\n",
    "    print(\"Reward:\", reward)\n",
    "    print(\"=\"*10)\n",
    "    \n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q. 조금 상황을 바꿔서 복도를 걸어다니는 취한 손님에 대한 환경을 구현해봅시다. 이 환경은 $\\texttt{CorridorEnv}$와 거의 동일하지만, 상태 변화에 확률이 추가됩니다. 에이전트는 똑같이 왼쪽 혹은 오른쪽으로 움직일 수 있지만, 이 에이전트는 취해있기 때문에 $20\\%$의 확률로 선택한 방향과 반대 방향으로 움직입니다. 이 취한 손님에 대한 환경 $\\texttt{DrunkenCorridorEnv}$를 구현해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Defines our corridor environment\n",
    "class DrunkenCorridorEnv:\n",
    "    def __init__(self):\n",
    "        rooms = list(range(101, 107))\n",
    "        self.state_space = [(i, j) for i in rooms for j in rooms]\n",
    "        self.action_space = [-1, 1, 0]\n",
    "\n",
    "    # 위의 코드와 동일!\n",
    "    def transition(self, state, action):\n",
    "        current_location, my_room = state\n",
    "        next_location = current_location + action \n",
    "        next_location = max(next_location, 101) \n",
    "        next_location = min(next_location, 106)\n",
    "        next_state = (next_location, my_room)\n",
    "        return next_state\n",
    "\n",
    "    # 위의 코드와 동일!\n",
    "    def reward_function(self, state, action):\n",
    "        next_state = transition(state, action)\n",
    "        next_location, my_room = next_state\n",
    "        if next_location == my_room:\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "    def reset(self):\n",
    "        same_start_and_end = False\n",
    "        while not same_start_and_end:\n",
    "            state = random.choice(self.state_space)\n",
    "            same_start_and_end = state[0] != state[1]\n",
    "        self.state = state\n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        if random.randint(0,10)<3:\n",
    "            next_state = self.transition(self.state, action)\n",
    "        else: \n",
    "            next_state = self.transition(self.state, action*-1)\n",
    "        reward = self.reward_function(self.state, action)\n",
    "        if action==0: done = (next_state[0]==next_state[1])\n",
    "        else: done=False\n",
    "        self.state = next_state\n",
    "        return next_state, reward, done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 수직선 위의 에이전트\n",
    "에이전트는 원점의 위치에서 출발하여 보상의 총합을 최대화 하도록 움직이고 싶어합니다. 매 순간 에이전트는 왼쪽 또는 오른쪽으로 이동할 수 있으며, 총 세 번 만 움직일 수 있습니다. 이 때마다 에이전트는 그 위치에 적혀있는 보상을 받습니다. 아래 코드를 통해 에이전트가 놓여있는 환경을 이해해 볼까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        self.observation_space = gym.spaces.Discrete(7, start=-3)\n",
    "        self.action_space = gym.spaces.Discrete(2)\n",
    "        self.num_steps = 0\n",
    "        \n",
    "    def reset(self):\n",
    "        state = 0\n",
    "        return state\n",
    "        \n",
    "    def step(self, action):\n",
    "        self.num_steps += 1\n",
    "\n",
    "        if action == 0:\n",
    "            next_state = state - 1\n",
    "        else:\n",
    "            next_state = state + 1\n",
    "        \n",
    "        if next_state > 3:\n",
    "            next_state = 3\n",
    "        elif next_state < -3:\n",
    "            next_state = -3\n",
    "        \n",
    "        reward = {\n",
    "            -3: 1,\n",
    "            -2: 1,\n",
    "            -1: 1,\n",
    "            0: 0,\n",
    "            1: -1,\n",
    "            2: -1,\n",
    "            3: 10\n",
    "        }[next_state]\n",
    "\n",
    "        done = self.num_steps >= 3\n",
    "        return next_state, reward, done, {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "근시안적인 관점에서 보면, 에이전트가 왼쪽으로 이동해야 당장 더 큰 보상을 받을 수 있습니다. 반대로 오른쪽으로 이동하게 되면 당장은 손해인 것처럼 보입니다. 하지만 총 세 번을 움직일 수 있는 상황에서는 오히려 두 번의 손해를 보고 나서야 비로소 가장 큰 보상을 받을 수 있게 됩니다. 즉, 장기적인 관점에서 에이전트는 보상의 총합을 최대화하기 위해 당장의 손해를 감수해야만 합니다. 이처럼 에이전트는 단순히 현재 상황에서의 최고의 선택을 고르는 것이 아니라 다음 상태까지 모두 고려한 최선의 선택을 내려야합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 무슨 환경일까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "Initial state: 0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym # type: ignore\n",
    "\n",
    "rewards=[]\n",
    "\n",
    "for i in range(500):\n",
    "    env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=False)\n",
    "    state, _ = env.reset()\n",
    "    print(\"Initial state:\", state)\n",
    "\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        # action = int(input(\"Choose action from (0, 1, 2, 3): \"))\n",
    "        action = random.choice([0,1,2,3])\n",
    "        # print(\"Chose action:\", action)\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        # print(\"New state:\", state)\n",
    "        # print(\"Reward:\", reward)\n",
    "        # print(\"=\"*10)[']\n",
    "\n",
    "    # print(\"Game over!\")\n",
    "    # print(\"Final state:\", state)\n",
    "    # print(\"Total reward:\", total_reward)\n",
    "    rewards.append(total_reward)\n",
    "    env.close()\n",
    "        \n",
    "print(max(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state: 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[99], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m total_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m---> 10\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mChoose action from (0, 1, 2, 3): \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChose action:\u001b[39m\u001b[38;5;124m\"\u001b[39m, action)\n\u001b[0;32m     12\u001b[0m     state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=True) # True로 바뀌면 어떻게 되나요?\n",
    "state, _ = env.reset()\n",
    "print(\"Initial state:\", state)\n",
    "\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "    action = int(input(\"Choose action from (0, 1, 2, 3): \"))\n",
    "    print(\"Chose action:\", action)\n",
    "    state, reward, done, _, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "    print(\"New state:\", state)\n",
    "    print(\"Reward:\", reward)\n",
    "\n",
    "print(\"Game over!\")\n",
    "print(\"Final state:\", state)\n",
    "print(\"Total reward:\", total_reward)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 나만의 강화 학습 환경 만들어보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEnv:\n",
    "    def __init__(self):\n",
    "        self.state_space = ???\n",
    "        self.action_space = ???\n",
    "\n",
    "    def transition(self, state, action):\n",
    "        next_state = ???\n",
    "        self.state = next_state\n",
    "        return next_state\n",
    "\n",
    "    def reward_function(self, state, action):\n",
    "        ???\n",
    "        return reward\n",
    "\n",
    "    def reset(self):\n",
    "        ???\n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        next_state = self.transition(self.state, action)\n",
    "        reward = self.reward_function(self.state, action)\n",
    "        done = ???\n",
    "        return next_state, reward, done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rllib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
